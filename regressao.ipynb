{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1645377711004_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-69-101-168.ec2.internal:20888/proxy/application_1645377711004_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-69-101-136.ec2.internal:8042/node/containerlogs/container_1645377711004_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.executor.instances': '10'}, 'proxyUser': 'jovyan', 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1645377711004_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-69-101-168.ec2.internal:20888/proxy/application_1645377711004_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-69-101-136.ec2.internal:8042/node/containerlogs/container_1645377711004_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \n",
    "    \"conf\":{\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.executor.instances\": \"10\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_csv(file_name):\n",
    "    df = (spark.read\n",
    "      .format(\"s3selectCSV\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(file_name))\n",
    "\n",
    "    df = df.na.drop()\n",
    "    df=df.withColumn(\"pressure\", df[\"pressure\"].cast(\"float\").alias(\"pressure\"))\n",
    "    df=df.withColumn(\"temperature\", df[\"temperature\"].cast(\"float\").alias(\"temperature\"))\n",
    "    df=df.withColumn(\"humidity\", df[\"humidity\"].cast(\"float\").alias(\"humidity\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    df = df.withColumn(\"relation\", col(\"humidity\") / col(\"temperature\"))\n",
    "\n",
    "    IQRdf = (df.agg(\n",
    "                 expr(\"percentile(relation , array(0.25))\")[0].alias(\"lower\"),\n",
    "                 expr(\"percentile(relation , array(0.75))\")[0].alias(\"upper\"),\n",
    "                 expr(\"percentile(relation , array(0.50))\")[0].alias(\"median\"))\n",
    "             .withColumn(\"deviation\", (col(\"upper\") - col(\"lower\"))/2))\n",
    "\n",
    "    df = (df.join(IQRdf)\n",
    "          .filter(abs(col(\"relation\") - col(\"median\")) >= (col(\"deviation\") * 2.2)))\n",
    "\n",
    "    df = (df.drop(\"lower\")\n",
    "          .drop(\"upper\")\n",
    "          .drop(\"median\")\n",
    "          .drop(\"deviation\"))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- relation: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- relation: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- relation: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- relation: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- relation: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- relation: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"_c0\", IntegerType(), True),\n",
    "    StructField(\"sensor_id\", IntegerType(), True),\n",
    "    StructField(\"location\", IntegerType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"pressure\", FloatType(), True),\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"humidity\", FloatType(), True),\n",
    "    StructField(\"relation\", DoubleType(), True),\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame([], schema)\n",
    "\n",
    "for i in range(2017, 2018):\n",
    "    for j in range(7, 13):\n",
    "        file_name = \"s3://sofia-air-quality-dataset/{}-{:02d}_bme280sof.csv\".format(i, j)\n",
    "        aux = get_dataframe_from_csv(file_name)\n",
    "        aux = remove_outliers(aux)\n",
    "        df = df.union(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_temp = df = df.withColumn(\"date\", to_date(col(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_temp = (df.groupby(\"date\")\n",
    "               .agg(\n",
    "                   avg(\"temperature\").alias(\"temperature\"),\n",
    "                   avg(\"humidity\").alias(\"humidity\"),\n",
    "                   avg(\"pressure\").alias(\"pressure\"))\n",
    "               .withColumn(\"relation\", col(\"humidity\") / col(\"temperature\"))\n",
    "               .orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+-----------------+-----------------+\n",
      "|      date|       temperature|         humidity|         pressure|         relation|\n",
      "+----------+------------------+-----------------+-----------------+-----------------+\n",
      "|2017-07-02|19.377313433594963|81.63064673883999|94004.93950171019|4.212691662267008|\n",
      "|2017-07-03|17.377409705492607|84.78500034402376|94656.82726140604|4.879035585909282|\n",
      "|2017-07-04| 16.74652957137202|76.00736052607147| 95123.0164612676|4.538693238030946|\n",
      "|2017-07-05|15.467403399257337| 74.0439253946078|95447.05046610169|4.787094736157395|\n",
      "|2017-07-06|15.867656325227305|74.86759029673512| 95290.9246314184|4.718251313378041|\n",
      "+----------+------------------+-----------------+-----------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_reg_temp.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------------------+-----------------+\n",
      "|            features|         relation|       temperature|         humidity|\n",
      "+--------------------+-----------------+------------------+-----------------+\n",
      "|[4.21269166226700...|4.212691662267008|19.377313433594963|81.63064673883999|\n",
      "|[4.87903558590928...|4.879035585909282|17.377409705492607|84.78500034402376|\n",
      "|[4.53869323803094...|4.538693238030946| 16.74652957137202|76.00736052607147|\n",
      "+--------------------+-----------------+------------------+-----------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
    "vecAssembler.setInputCols([\"relation\", \"temperature\", \"humidity\"])\n",
    "vdf = vecAssembler.transform(df_reg_temp)\n",
    "vdf = vdf.select([\"features\", \"relation\", \"temperature\", \"humidity\"])\n",
    "vdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vdf.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9966132411957674,0.0,0.0]\n",
      "Intercept: 0.017657884882711774"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = \"features\", labelCol=\"relation\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.299797\n",
      "r2: 0.999989"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|          relation|       temperature|          humidity|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|               122|               122|               122|\n",
      "|   mean| 5.213800540104589|-3.215223890587103| 82.53706275648601|\n",
      "| stddev| 88.88530181186259| 33.55123041843445|  9.09932393079846|\n",
      "|    min|-344.7366089500445|           -142.75|56.456836517399815|\n",
      "|    max| 487.8574307414297|20.910073170496297|             100.0|\n",
      "+-------+------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|            features|         relation|       temperature|         humidity|       prediction|\n",
      "+--------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|[4.21269166226700...|4.212691662267008|19.377313433594963|81.63064673883999|4.216082176573019|\n",
      "|[4.87903558590928...|4.879035585909282|17.377409705492607|84.78500034402376|4.880169354065251|\n",
      "|[4.78709473615739...|4.787094736157395|15.467403399257337| 74.0439253946078| 4.78853988579573|\n",
      "|[4.24747456791105...| 4.24747456791105|17.773345070825496|75.49183117503851|4.250747280905135|\n",
      "|[4.09358694552932...|4.093586945529321|19.011199951171875|77.82399993896485| 4.09738083878337|\n",
      "+--------------------+-----------------+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-----------------+--------------------+\n",
      "|       prediction|         relation|            features|\n",
      "+-----------------+-----------------+--------------------+\n",
      "|4.216082176573019|4.212691662267008|[4.21269166226700...|\n",
      "|4.880169354065251|4.879035585909282|[4.87903558590928...|\n",
      "| 4.78853988579573|4.787094736157395|[4.78709473615739...|\n",
      "|4.250747280905135| 4.24747456791105|[4.24747456791105...|\n",
      "| 4.09738083878337|4.093586945529321|[4.09358694552932...|\n",
      "+-----------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.999988"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.show(n=5)\n",
    "lr_predictions.select(\"prediction\", \"relation\", \"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"relation\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.444643"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.40102463824382684, 0.00313344096521352, 0.003053440427515304, 0.003044413005872045, 0.0030444099591633213, 0.0030444099591632233]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.00228642559909...|\n",
      "|-0.00167830570654...|\n",
      "|-0.00345024444580...|\n",
      "|-0.00285680856605...|\n",
      "|-0.00305623382280...|\n",
      "|-7.27542546576032...|\n",
      "|-9.03043374882450...|\n",
      "|-0.00145045441450...|\n",
      "|-7.81172177928724E-4|\n",
      "|0.013850198106787559|\n",
      "|-0.02674376809498...|\n",
      "|-0.02636471933063...|\n",
      "|-0.02432425794905...|\n",
      "|-0.02103869089806...|\n",
      "|-0.02080963878800468|\n",
      "|-0.00320971932896...|\n",
      "|-0.00433729530668...|\n",
      "|-0.00312124115563...|\n",
      "|-0.00327323385276...|\n",
      "|-0.00233209749089...|\n",
      "+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|         prediction|           relation|            features|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|  4.216082176573019|  4.212691662267008|[4.21269166226700...|\n",
      "|  4.880169354065251|  4.879035585909282|[4.87903558590928...|\n",
      "|   4.78853988579573|  4.787094736157395|[4.78709473615739...|\n",
      "|  4.250747280905135|   4.24747456791105|[4.24747456791105...|\n",
      "|   4.09738083878337|  4.093586945529321|[4.09358694552932...|\n",
      "|  4.209623468031279|  4.206211005303238|[4.20621100530323...|\n",
      "|  4.660072386436954|  4.658190669817039|[4.65819066981703...|\n",
      "|  4.398058120809037|  4.395286009516174|[4.39528600951617...|\n",
      "|  4.355782787383221|  4.352867013181054|[4.35286701318105...|\n",
      "|   4.30011082122218|  4.297005858763474|[4.29700585876347...|\n",
      "| 14.689088428138156| 14.721287994982093|[14.7212879949820...|\n",
      "|-25.784419959813867|-25.889760218054548|[-25.889760218054...|\n",
      "|-1.9270131649691333|-1.9512795630916648|[-1.9512795630916...|\n",
      "|  4.215910648103976| 4.2125195508982705|[4.21251955089827...|\n",
      "|  4.200545693363853|  4.197102381925393|[4.19710238192539...|\n",
      "| 4.3526704204245465| 4.3497440695655945|[4.34974406956559...|\n",
      "|  4.149902340748434|  4.146286929629519|[4.14628692962951...|\n",
      "|  4.461417299284577|  4.458860499455239|[4.45886049945523...|\n",
      "|  4.619804006450412|  4.617785447086678|[4.61778544708667...|\n",
      "|  4.179469295558123|  4.175954360873171|[4.17595436087317...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"relation\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 64.4116"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'relation')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=\"relation\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseVector(3, {0: 0.925, 1: 0.0394, 2: 0.0356})"
     ]
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(date=datetime.date(2017, 7, 2), temperature=19.377313433594963, humidity=81.63064673883999, pressure=94004.93950171019, relation=4.212691662267008)]"
     ]
    }
   ],
   "source": [
    "df_reg_temp.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+--------------------+\n",
      "|       prediction|         relation|            features|\n",
      "+-----------------+-----------------+--------------------+\n",
      "|4.378995403194841|4.212691662267008|[4.21269166226700...|\n",
      "|4.804887123086766|4.879035585909282|[4.87903558590928...|\n",
      "|4.596032991956885|4.787094736157395|[4.78709473615739...|\n",
      "|4.620139566424155| 4.24747456791105|[4.24747456791105...|\n",
      "|4.378995403194841|4.093586945529321|[4.09358694552932...|\n",
      "+-----------------+-----------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'relation', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'relation', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 65.2448"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"relation\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
